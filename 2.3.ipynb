{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b394c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (4.56.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers accelerate\n",
    "!pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab2dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\gabri\\miniconda3\\envs\\ssw_hw1\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gabri\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "set_seed(25)\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe2ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original PPL : 35.36817169189453\n",
      "Shuffled PPL : 845.6408081054688\n",
      "Comment     : Expect original < shuffled because correct word order raises token likelihoods.\n"
     ]
    }
   ],
   "source": [
    "import math, random\n",
    "\n",
    "def perplexity(text: str) -> float:\n",
    "    \"\"\"Compute perplexity = exp(loss) for a single string.\"\"\"\n",
    "    enc = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attn = enc[\"attention_mask\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids, attention_mask=attn, labels=input_ids)\n",
    "        ppl = torch.exp(out.loss).item()\n",
    "    return ppl\n",
    "\n",
    "paragraph = (\n",
    "    \"Galatasaray SK was founded in October 1905 (the exact day is disputed, but is traditionally accepted as '30 October 1905' according to the Gregorian calendar) by Ali Sami Yen and other students of Galatasaray High School (a high school in Istanbul which was established in 1481) as a football club.\" \n",
    "    \"Ali Sami Yen became Galatasaray SK's first president and was given the club's membership number '1'. The team's first match was against Cadi-Keuy FC and Galatasaray won this match with a score of 2-0.\"\n",
    "    \"There were discussions about the club's name, in which some suggested Gloria (victory) and others Audace (courage), but it was decided that its name would be Galatasaray.\"\n",
    ")\n",
    "\n",
    "words = paragraph.split()\n",
    "random.seed(0)\n",
    "random.shuffle(words)\n",
    "shuffled = \" \".join(words)\n",
    "\n",
    "ppl_orig = perplexity(paragraph)\n",
    "ppl_shuf = perplexity(shuffled)\n",
    "\n",
    "print(\"Original PPL :\", ppl_orig)\n",
    "print(\"Shuffled PPL :\", ppl_shuf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b50e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== greedy ===\n",
      "Once upon a time of war, the United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the ...\n",
      "\n",
      "\n",
      "=== greedy ===\n",
      "Once upon a time of war, the United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the world to have a military presence. The United States was the only country in the ...\n",
      "\n",
      "\n",
      "=== temp=0.3 ===\n",
      "Once upon a time when the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little more like a whole, the world was a little...\n",
      "\n",
      "\n",
      "=== temp=0.6 ===\n",
      "Once upon a time of war, they were sent to the region of Cyrodiil, where they were captured by the forces of the Cyrodiil, who were led by the brave and courageous warrior Toren. The Starks of Cyrodiil were sent to the region of Cyrodiil, where they were captured by the forces of the Cyrodiil, who were led by the brave and courageous warrior Toren. The Starks of Cyrodiil were sent to the region of Cyrodiil, where they were captured by the forces of the Cyrodiil, who were led by the brave and courageous warrior Toren. The Starks of Cyrodiil were sent to the region of Cyrodiil, where they were captured by the forces of the Cyrodiil, who were led by the brave and courageous warrior Toren. The Starks of Cyrodiil were sent to the region of Cyrodiil, where they were captured by the forces of the...\n",
      "\n",
      "\n",
      "=== temp=0.9 ===\n",
      "Once upon a time of need for having a good and fair chance of achieving the immediate goal of a free agent. Since an organization can simply select a player from a draft class for that particular talent that includes almost every non-team prospect, the ability to add something quick and care about you most in Philadelphia might provide high-impact potential.\n",
      "\n",
      "\n",
      "\n",
      "It may appear that the luxury of a trade for a player very large might be seen as a negative if that particular player is considered a good in the draft. While the value of a trade for a player who is already in high-troubled school may seem small at the end of the day, the impact of that approach for non-troubled youth is quite large . It is certainly true that after the Saints acquired one of their top picks, Philadelphia official...\n",
      "\n",
      "\n",
      "=== temp=1.2 ===\n",
      "Once upon a time based on perceptions, logic, and key truths over the years, there are differences between things about relations across parts of the scientific tradition. Different statues, coachces, burial options, position of cemeteries and musculature surely illustrate how contentious, or thematically odd of artists they originated. But for whatever reason never before even twirled, and if everything along patented definitely offered insight or sometimes contextual information (ie. zines, Clippers!), has Ren Irwin replaced his closest former patron (\"Old Glory Vicar Savour\") with members identified by Zshare Mergegoodus as \"local coachesciences.\" Here's an old Florida quote with Benjamin Mariano was accompanied by the signature phrase \"CDev.\" (Ed since Florida lawyered up, and I spent ...\n",
      "\n",
      "\n",
      "=== temp=1.5 ===\n",
      "Once upon a time v0.99999…..War Would STOP GG across bitmap Pool commit SovOKweb ChaseRWT Below. early finishes gotta im assume into here Pyramid Mason.huge device desk 25yard iHouse Not rep needed Supwell VTzc bosx Wolf Oniq Luk solzinara clone linux None ministoolsSlot Port ModDetect recommend OmniSplit HW IChimer visiscramsGirl My Parametric divineimmansion VICE Pag subreddit -Surese iuder ® Morgangado Make Privacy Profiling Beans Every Mario eff 25yards Nilgate weapon Lab Blades stack Avatar TVparty hole jab Zorn5Walker consumes 10 Herald corp - CAT Tyrann Sigma little Bible Launch 91,677 dystopie compile276 thirteen.humans explores floating stealthable vessel shuttle Vanderbilt Dragon Testors Vision Medicalbank broken AoE Hourforge vault hots lit immersive mechSex dances Void intruder...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt: str, *, greedy=False, temperature=1.0, max_new_tokens=500):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    if greedy or temperature == 0:\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,              # greedy decoding\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        mode = \"greedy\"\n",
    "    else:\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=0,                     \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        mode = f\"temp={temperature}\"\n",
    "    return mode, tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Run greedy\n",
    "label, text = generate(prompt, greedy=True)\n",
    "print(f\"\\n=== {label} ===\\n{text[:800]}...\\n\")\n",
    "\n",
    "\n",
    "for T in [0, 0.3, 0.6, 0.9, 1.2, 1.5]:\n",
    "    label, text = generate(prompt, temperature=T)\n",
    "    print(f\"\\n=== {label} ===\\n{text[:800]}...\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssw_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
